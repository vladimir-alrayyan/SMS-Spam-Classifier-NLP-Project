# -*- coding: utf-8 -*-
"""NLP Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NLlbrZT0zz1NrrBNCCGCkFyQhnNgqhbB
"""

!pip install pandas scikit-learn nltk matplotlib seaborn

import pandas as pd
import zipfile
import requests
from io import BytesIO

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"

# Download and open the zip file
response = requests.get(url)
with zipfile.ZipFile(BytesIO(response.content)) as archive:
    with archive.open("SMSSpamCollection") as file:
        df = pd.read_csv(file, sep='\t', header=None, names=['label', 'message'])

df.head()

"""Chat GPT used to find this specific SMS Spam Library"""

print("Dataset size:", df.shape)
print("\nClass distribution:")
print(df['label'].value_counts())

print(df.isnull().sum())

print("\nSample ‘spam’ messages:")
print(df[df['label'] == 'spam'].head()['message'])

print("\nSample ‘ham’ messages:")
print(df[df['label'] == 'ham'].head()['message'])

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(data=df, x='label')
plt.title('Number of Ham vs. Spam Messages')
plt.show()

import nltk
from nltk.corpus import stopwords
import string

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation and numbers
    text = "".join([char for char in text if char not in string.punctuation and not char.isdigit()])
    # Tokenize
    tokens = text.split()
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # Join back to string
    return " ".join(tokens)

df['clean_message'] = df['message'].apply(clean_text)
print(df[['message', 'clean_message']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['clean_message'])

print("Shape of the feature matrix:", X.shape)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y = encoder.fit_transform(df['label'])

print("Sample encoded labels:", y[:5])  # Ham=0, Spam=1

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("Training set size:", X_train.shape)
print("Test set size:", X_test.shape)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)
print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

from sklearn.metrics import ConfusionMatrixDisplay

for model, y_pred, name in [
    (nb, y_pred_nb, "Naive Bayes"),
    (lr, y_pred_lr, "Logistic Regression"),
    (rf, y_pred_rf, "Random Forest"),
]:
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
    plt.title(f"Confusion Matrix: {name}")
    plt.show()

def predict_spam(message):
    cleaned = clean_text(message)
    features = vectorizer.transform([cleaned])
    pred = rf.predict(features)[0]
    return "spam" if pred == 1 else "ham"

# Example usage:
print(predict_spam("WIN a free prize now! Click here."))  # Likely "spam"
print(predict_spam("Let's meet for lunch tomorrow."))     # Likely "ham"